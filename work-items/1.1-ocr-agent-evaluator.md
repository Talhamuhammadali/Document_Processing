# Work Item 1.1: OCR Agent Evaluator

## Overview

**What:** Build a LangGraph agent that systematically tests multiple OCR engines (Tesseract, EasyOCR, PaddleOCR, RapidOCR) on sample documents and evaluates their performance using an LLM as a judge.

**Why:** Understanding the tradeoffs between different OCR engines (accuracy vs speed vs complexity) is fundamental to building effective document processing pipelines. An automated evaluation system with LLM-based quality assessment provides both quantitative and qualitative insights.

**Deliverable:** 
- Working LangGraph agent that processes test documents through all OCR engines
- LLM-based evaluation system that scores OCR quality
- Markdown reports comparing engine performance
- Langfuse traces showing execution flow and timing

**Estimated Time:** 6-8 hours

**Prerequisites:** 
- Langfuse Docker setup running
- Test documents prepared (PDFs/images at varying difficulty levels)

---

## Architecture

```
┌─────────────────────────────────────────────────┐
│  OCR Evaluation Agent (LangGraph)              │
├─────────────────────────────────────────────────┤
│  1. Document Loader Node                       │
│     ↓ Load test PDFs/images                    │
│  2. OCR Runner Node (Parallel)                 │
│     ├→ Tesseract OCR                           │
│     ├→ EasyOCR                                 │
│     ├→ PaddleOCR                               │
│     └→ RapidOCR                                │
│     ↓ Collect all outputs                      │
│  3. Ground Truth Comparison Node               │
│     ↓ Compare against known text (if available)│
│  4. LLM Judge Node                             │
│     ↓ Evaluate quality, readability, accuracy  │
│  5. Report Generator Node                      │
│     └→ Create comparison matrix + visuals      │
└─────────────────────────────────────────────────┘
         ↓
    Langfuse Tracing (track each step)
```

---

## Sub-Tasks

### Task 1.1.1: Setup Test Dataset ⏳
**Time:** 30 minutes  
**Priority:** High

**Goal:** Create a structured test dataset with documents at varying difficulty levels.

**Implementation Steps:**
1. Create `data/ocr_test_dataset.py` with `TestDocument` Pydantic model
2. Prepare test documents in `data/test_documents/`:
   - Easy: Clean printed invoice/document
   - Medium: PDF with complex table layout
   - Hard: Handwritten form or low-quality scan
3. Define ground truth text for documents where available

**Files to Create:**
- `data/ocr_test_dataset.py`
- `data/test_documents/` (directory with sample files)

**Success Criteria:**
- [ ] `TestDocument` Pydantic model with `id`, `filepath`, `difficulty`, `ground_truth`, `description`
- [ ] At least 3 test documents covering easy/medium/hard cases
- [ ] `TEST_DOCUMENTS` list populated with test cases

**Learning Outcome:** Understand importance of systematic evaluation with diverse test cases.

---

### Task 1.1.2: Create OCR Engine Wrappers ⏳
**Time:** 1-1.5 hours  
**Priority:** High

**Goal:** Build unified interface for all OCR engines with standardized output format.

**Implementation Steps:**
1. Create `app/ocr/engines.py` with:
   - `OCRResult` Pydantic model (text, confidence, processing_time, metadata)
   - `OCREngine` abstract base class
   - Concrete implementations for:
     - `TesseractEngine`
     - `EasyOCREngine`
     - `PaddleOCREngine`
     - `RapidOCREngine`

2. Each wrapper should:
   - Load/initialize the engine
   - Process image and extract text
   - Calculate confidence scores (normalize to 0-1 range)
   - Track processing time

**Files to Create:**
- `app/ocr/__init__.py`
- `app/ocr/engines.py`

**Success Criteria:**
- [ ] All 4 engines implement `OCREngine.process()` method
- [ ] Consistent `OCRResult` output from all engines
- [ ] Processing time tracked accurately
- [ ] Confidence scores normalized across engines

**Learning Outcome:** Understand how different OCR libraries work and their API differences.

**Code Reference:**
```python
class OCRResult(BaseModel):
    engine: str
    text: str
    confidence: float | None
    processing_time: float
    metadata: dict = {}

class OCREngine(ABC):
    @abstractmethod
    def process(self, image_path: Path) -> OCRResult:
        pass
```

---

### Task 1.1.3: Build LLM Judge for Quality Assessment ⏳
**Time:** 1-1.5 hours  
**Priority:** High

**Goal:** Create an LLM-based evaluator that assesses OCR output quality on multiple dimensions.

**Implementation Steps:**
1. Create `app/ocr/evaluator.py` with:
   - `OCRQualityScore` Pydantic model with fields:
     - `readability` (1-10)
     - `structure_preservation` (1-10)
     - `completeness` (1-10)
     - `accuracy` (1-10)
     - `overall_score` (1-10)
     - `reasoning` (explanation text)
   
2. Implement `OCREvaluator` class:
   - Initialize Claude/Anthropic LLM
   - `evaluate()` method that takes OCR text + optional ground truth
   - Build evaluation prompt that asks LLM to score on criteria
   - Use structured output to get `OCRQualityScore`

3. Add `@observe` decorator for Langfuse tracing

**Files to Create:**
- `app/ocr/evaluator.py`

**Success Criteria:**
- [ ] `OCREvaluator.evaluate()` returns structured scores
- [ ] Evaluation prompt includes all quality dimensions
- [ ] Ground truth comparison works when available
- [ ] Langfuse traces show LLM evaluation calls

**Learning Outcome:** Learn how to use LLMs for qualitative evaluation and structured output extraction.

**Key Concepts:**
- Structured output with Pydantic
- LLM-as-a-judge pattern
- Prompt engineering for evaluation

---

### Task 1.1.4: Implement LangGraph Evaluation Agent ⏳
**Time:** 2-2.5 hours  
**Priority:** High

**Goal:** Build a multi-step LangGraph workflow that orchestrates OCR processing and evaluation.

**Implementation Steps:**
1. Create `app/ocr/agent.py` with:
   - `OCRAgentState` TypedDict defining workflow state
   - `OCREvaluationAgent` class

2. Define workflow nodes:
   - `run_ocr`: Execute all OCR engines in parallel/sequence
   - `evaluate_results`: Run LLM judge on each OCR output
   - `generate_report`: Format results into markdown report

3. Build LangGraph:
   - Set entry point to `run_ocr`
   - Add edges: `run_ocr` → `evaluate_results` → `generate_report`
   - Compile graph

4. Add `@observe` decorators on each node for Langfuse tracing

5. Implement `_format_report()` to create comparison table:
   - Performance summary (speed, confidence, overall score)
   - Detailed quality scores per engine
   - LLM reasoning for each evaluation

**Files to Create:**
- `app/ocr/agent.py`

**Success Criteria:**
- [ ] LangGraph workflow executes all steps sequentially
- [ ] State passes correctly between nodes
- [ ] All OCR engines are executed and results collected
- [ ] LLM evaluations run for each engine
- [ ] Markdown report generated with comparison table
- [ ] Langfuse shows complete workflow trace with all nodes

**Learning Outcome:** Master LangGraph for building multi-step agent workflows with proper state management.

**Graph Structure:**
```
START → run_ocr → evaluate_results → generate_report → END
```

---

### Task 1.1.5: Create Runner Script & Generate Reports ⏳
**Time:** 30 minutes  
**Priority:** Medium

**Goal:** Build executable script to run evaluations and save results.

**Implementation Steps:**
1. Create `scripts/run_ocr_evaluation.py`:
   - Initialize Langfuse client
   - Load test documents from dataset
   - Instantiate `OCREvaluationAgent`
   - Loop through documents and run evaluations
   - Save markdown reports to `reports/` directory
   - Print summary statistics

2. Create `reports/` directory for output

3. Run evaluation on all test documents

**Files to Create:**
- `scripts/run_ocr_evaluation.py`
- `reports/` (directory)

**Success Criteria:**
- [ ] Script runs without errors
- [ ] Markdown report generated for each test document
- [ ] Reports saved to `reports/{doc_id}_evaluation.md`
- [ ] Langfuse dashboard shows traces for all evaluations
- [ ] Console output shows progress and results

**Learning Outcome:** Understand end-to-end execution and observability integration.

---

### Task 1.1.6: Analysis & Documentation ⏳
**Time:** 1 hour  
**Priority:** Medium

**Goal:** Analyze results and document findings.

**Implementation Steps:**
1. Review generated reports and identify patterns:
   - Which engine is fastest?
   - Which has best accuracy on clean documents?
   - Which handles tables/layouts best?
   - Which works on handwritten text?

2. Check Langfuse dashboard:
   - Compare execution times per engine
   - Review LLM judge reasoning
   - Identify bottlenecks

3. Create `docs/ocr-comparison.md` with:
   - Summary table of engine characteristics
   - Speed vs accuracy tradeoffs
   - Recommended use cases for each engine
   - Sample outputs showing differences
   - Decision matrix for engine selection

**Files to Create:**
- `docs/ocr-comparison.md`

**Success Criteria:**
- [ ] Comparative analysis completed
- [ ] Langfuse traces reviewed for performance insights
- [ ] Documentation created with actionable recommendations
- [ ] Decision made on primary OCR engine for your pipeline

**Learning Outcome:** Learn to analyze evaluation results and make informed technical decisions.

---

## Optional Extensions

### Extension A: Add Docling as 5th Engine
**Time:** +1 hour  
**Priority:** Low

Create `DoclingEngine` wrapper that uses RT-DETR + TableFormer + OCR pipeline. Compare against standalone OCR engines.

**Why:** See if the multi-model pipeline provides better results than single OCR engines.

**Files to Modify:**
- `app/ocr/engines.py` (add `DoclingEngine` class)
- `app/ocr/agent.py` (add Docling to engine list)

---

### Extension B: Add Quantitative Metrics
**Time:** +1 hour  
**Priority:** Low

For documents with ground truth, calculate:
- Character Error Rate (CER)
- Word Error Rate (WER)  
- BLEU/ROGUE scores

Add these to the evaluation report alongside LLM scores.

**Why:** Complement qualitative LLM assessment with standard quantitative metrics.

**Files to Create:**
- `app/ocr/metrics.py`

---

### Extension C: Visualization Dashboard
**Time:** +1.5 hours  
**Priority:** Low

Create notebook with:
- Speed vs accuracy scatter plot
- Radar chart showing quality dimensions per engine
- Confidence distribution histograms

**Why:** Visual comparison makes insights more accessible and shareable.

**Files to Create:**
- `experiments/ocr_comparison_viz.ipynb`

---

## Notes & Tips

**Debugging Tips:**
- Test each OCR engine individually first before running agent
- Start with 1 test document, then scale to full dataset
- Check Langfuse traces if agent gets stuck at any node

**Common Issues:**
- EasyOCR/PaddleOCR require model downloads on first run (may take time)
- Some engines may fail on certain image formats (add error handling)
- LLM evaluation can be slow (consider caching if re-running)

**Environment Variables Needed:**
```bash
ANTHROPIC_API_KEY=your_key_here
LANGFUSE_SECRET_KEY=your_key_here
LANGFUSE_PUBLIC_KEY=your_key_here
LANGFUSE_HOST=http://localhost:3000
```

**Next Steps After Completion:**
- Use learnings to choose primary OCR engine for Work Item 1.2
- Apply similar agent pattern to other evaluation tasks
- Expand test dataset based on your actual document types

---

## Acceptance Criteria

This work item is complete when:
- ✅ All 4 OCR engines successfully process test documents
- ✅ LLM judge provides quality scores for all outputs
- ✅ LangGraph agent executes without errors
- ✅ Markdown reports generated for all test documents  
- ✅ Langfuse dashboard shows complete execution traces
- ✅ Analysis document created with engine recommendations
- ✅ You can explain the tradeoffs between OCR engines

---

## Related Work Items

- **Next:** [Work Item 1.2: Docling Pipeline Deep Dive](./1.2-docling-deep-dive.md)
- **Prerequisite for:** Work Item 2.1 (Chunking Strategy Experiments)
